{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 做词性预测\n",
    "前面我们讲了词嵌入以及 n-gram 模型做单词预测，但是目前还没有用到 RNN，在最后这一次课中，我们会结合前面讲的所有预备知识，教大家如何使用 LSTM 来做词性预测。\n",
    "\n",
    "## 模型介绍\n",
    "对于一个单词，会有这不同的词性，首先能够根据一个单词的后缀来初步判断，比如 -ly 这种后缀，很大概率是一个副词，除此之外，一个相同的单词可以表示两种不同的词性，比如 book 既可以表示名词，也可以表示动词，所以到底这个词是什么词性需要结合前后文来具体判断。\n",
    "\n",
    "根据这个问题，我们可以使用 lstm 模型来进行预测，首先对于一个单词，可以将其看作一个序列，比如 apple 是由 a p p l e 这 5 个单词构成，这就形成了 5 的序列，我们可以对这些字符构建词嵌入，然后输入 lstm，就像 lstm 做图像分类一样，只取最后一个输出作为预测结果，整个单词的字符串能够形成一种记忆的特性，帮助我们更好的预测词性。\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcgy1fmxi67w0f7j30ap05qq2u.jpg)\n",
    "\n",
    "接着我们把这个单词和其前面几个单词构成序列，可以对这些单词构建新的词嵌入，最后输出结果是单词的词性，也就是根据前面几个词的信息对这个词的词性进行分类。\n",
    "\n",
    "下面我们用例子来简单的说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils.layers import lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用下面简单的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(\"The dog ate the apple\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), \n",
    "                  [\"NN\", \"V\", \"DET\", \"NN\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要对单词和标签进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "tags = []\n",
    "for context, tag in training_data:\n",
    "    for w in context:\n",
    "        words.append(w.lower())\n",
    "    for t in tag:\n",
    "        tags.append(t.lower())\n",
    "words = list(set(words))\n",
    "tags = list(set(tags))\n",
    "\n",
    "word_to_idx = dict(zip(words, range(len(words))))\n",
    "tag_to_idx = dict(zip(tags, range(len(tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们对字母进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "chars = list(alphabet)\n",
    "\n",
    "char_to_idx = dict(zip(chars, range(len(chars))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.contrib.lookup.index_table_from_tensor`\n",
    "在`tensorflow`运行过程中, 我们无法使用`python`下的字典, 因为图的元素是一个张量而不是具体值, `python`无法返回.\n",
    "\n",
    "我们需要用到`tf.contrib.lookup.index_table_from_tensor`, 它能帮我们搭建从字符串到编码的映射关系\n",
    "\n",
    "- 首先, 根据映射关系构造一个`table`\n",
    "\n",
    "函数定义非常简单, 参数就是需要映射元素列表的`tensor`形式, 在这里我们设置为常量`tensor`. 这样列表中元素的每一项都被映射成自己的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(words))\n",
    "tag_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(tags))\n",
    "char_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立`table`之后, 我们就可以输入一个对应映射关系的列表, 从而查找到它的下标.\n",
    "\n",
    "- 构建占位符, 等待填入元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ph = tf.placeholder(tf.string, [None,])\n",
    "tag_ph = tf.placeholder(tf.string, [None,])\n",
    "char_ph = tf.placeholder(tf.string, [None,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lookup`\n",
    "\n",
    "- 调用`table`的`lookup`方法, 就可以找到对应的下标了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_code = word_table.lookup(word_ph)\n",
    "tag_code = tag_table.lookup(tag_ph)\n",
    "char_code = char_table.lookup(char_ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看实际效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(symbols):\n",
    "    return map(lambda x: x.lower(), symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 当定义了`table`形式的`tensor`后, 我们需要额外对这些`table`初始化一次, 非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填入一个单词, 查看每个字母对应的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(char_code, feed_dict={char_ph: lower('apple')}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填入一个句子, 查看每个单词对应的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1][0])\n",
    "print(sess.run(word_code, feed_dict={word_ph: lower(training_data[1][0])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`seq-lstm`模型\n",
    "- 首先构建单个字符的 lstm 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_lstm(char_code, n_char, char_dim, char_hidden, scope='char_lstm', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 嵌入\n",
    "        embeddings = tf.get_variable('embeddings', shape=(n_char, char_dim), \n",
    "                                          dtype=tf.float32, initializer=tf.random_uniform_initializer(minval=0.0, maxval=1.0))\n",
    "        char_embed = tf.nn.embedding_lookup(embeddings, char_code, name='embed')\n",
    "        \n",
    "        # 将输入满足`(seq, batch, feature)`条件， 这里`batch=1`\n",
    "        char_embed = tf.expand_dims(char_embed, axis=1)\n",
    "        \n",
    "        # 经过`lstm`网络给出特征\n",
    "        out, _ = lstm(char_embed, char_hidden, 1, 1)\n",
    "        \n",
    "    return out[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造词性分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_tagger(word_code, word_list, n_word, n_char, word_dim, char_dim, \n",
    "               word_hidden, char_hidden, n_tag, scope='lstm_tagger', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 首先对一个句子里的所有单词用`char_lstm`进行编码\n",
    "        def char_lstm_fun(single_word):\n",
    "            # 使用`tf.string_split`对单词进行字母级别的分割\n",
    "            char_list = tf.string_split([single_word], delimiter='').values\n",
    "            \n",
    "            # 使用`char_table`查找所有字母的编码\n",
    "            char_code = char_table.lookup(char_list)\n",
    "            \n",
    "            # 将编码进入`lstm`得到输出\n",
    "            char_lstm_out = char_lstm(char_code, len(chars), 10, char_hidden)\n",
    "\n",
    "            return char_lstm_out\n",
    "        \n",
    "        # tf.while_loop退出循环的条件\n",
    "        def cond(i, char_out_list, word_list):\n",
    "            return tf.less(i, tf.shape(word_list)[0])\n",
    "        \n",
    "        # tf.while_loop循环体\n",
    "        def body(i, char_out_list, word_list):\n",
    "            char_out = char_lstm_fun(word_list[i])\n",
    "            # 如果不是第一个, 在第1维进行 concate, 否则直接赋值\n",
    "            char_out_list = tf.cond(i > 0, lambda: tf.concat([char_out_list, char_out], axis=0), lambda: char_out)\n",
    "            \n",
    "            return i + 1, char_out_list, word_list\n",
    "        \n",
    "        # tf.while_loop的初始变量 i\n",
    "        i = tf.constant(0)\n",
    "        \n",
    "        # tf.while_loop的初始变量 init_char_list\n",
    "        ## 为了更少debug, 用一个形状为 (None, char_hidden) 的 placeholder 作为初始化, 这样\n",
    "        ## 经过 tf.while_loop 之后形状不会发生改变\n",
    "        ## 但这增加了一个实际上没有用的 placeholder, 或许有更优的解决方法\n",
    "        init_char_list = tf.placeholder(tf.float32, shape=(None, char_hidden))\n",
    "        \n",
    "        # tf.while_loop\n",
    "        # 三个参数, 第一个是退出循环条件函数, 第二个是循环体函数, 第三个是带入的初始变量\n",
    "        # 可以参考 https://blog.csdn.net/qq_20611245/article/details/77363609\n",
    "        _, char_out_list, _ = tf.while_loop(cond, body, [i, init_char_list, word_list])\n",
    "        \n",
    "        # 最后将形状从 (seq, char_hidden) --> (seq, 1, char_hidden)\n",
    "        char_out = tf.expand_dims(char_out_list, axis=1)\n",
    "        \n",
    "        # 构造单词的嵌入模型\n",
    "        word_embeddings = tf.get_variable('embeddings', shape=(n_word, word_dim), \n",
    "                                          dtype=tf.float32, initializer=tf.random_uniform_initializer(minval=0.0, maxval=1.0))\n",
    "        word_embed = tf.nn.embedding_lookup(word_embeddings, word_code, name='word_embed')# (seq, word_dim)\n",
    "        word_embed = tf.expand_dims(word_embed, axis=1) # (seq, 1, word_dim)\n",
    "        \n",
    "        # 将单词的嵌入向量和单词的`lstm`结果按照最后一维(特征)进行连接\n",
    "        net = tf.concat([char_out, word_embed], axis=-1)\n",
    "        \n",
    "        # 进入`lstm`\n",
    "        net, _ = lstm(net, word_hidden, 1, 1)\n",
    "        \n",
    "        # 分类层\n",
    "        net = tf.reshape(net, (-1, word_hidden))\n",
    "        net = slim.fully_connected(net, n_tag, activation_fn=None, scope='classification')\n",
    "        \n",
    "        return net, init_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net, init_char_list = lstm_tagger(word_code, word_ph, len(words), len(chars), 100, 10, 128, 50, len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=tag_code, logits=net)\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(1e-2, 0.9)\n",
    "\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(300):\n",
    "    train_loss = 0\n",
    "    for word, tag in training_data:\n",
    "        curr_train_loss, _ = sess.run([loss, train_op], feed_dict={word_ph: lower(word), tag_ph: lower(tag), init_char_list: [[1] * 50]})\n",
    "        train_loss += curr_train_loss\n",
    "    if (e + 1) % 50 == 0:\n",
    "        print('Epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = 'Everybody ate the apple'\n",
    "out = sess.run(net, feed_dict={word_ph: lower(test_sent.split()), init_char_list: [[1] * 50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后可以得到上面的结果，因为最后一层的线性层没有使用 softmax，所以数值不太像一个概率，但是每一行数值最大的就表示属于该类，可以看到第一个单词 'Everybody' 属于 nn，第二个单词 'ate' 属于 v，第三个单词 'the' 属于det，第四个单词 'apple' 属于 nn，所以得到的这个预测结果是正确的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

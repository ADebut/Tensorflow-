{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。\n",
    "\n",
    "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n",
    "\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data(data_dir='cifar10_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从`utils/`中可以使用我们之前定义过的各种接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layers import conv, max_pool, fc\n",
    "from utils.learning import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vgg`是一个不断堆叠的网络结构, 我们从它重复的最小单元写起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(inputs, num_convs, out_depth, scope='vgg_block', reuse=None):\n",
    "    \"\"\"构建vgg_block.\n",
    "    \n",
    "    一个 vgg_block 由`num_convs`个卷积层和一个最大值池化层构成.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 这一个block里卷积层的个数\n",
    "        out_depth: 每一个卷积层的卷积核个数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    in_depth = inputs.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        # 循环定义`num_convs`个卷积层\n",
    "        for i in range(num_convs):\n",
    "            net = conv(net, ksize=[3, 3], out_depth=out_depth, strides=[1, 1], padding='SAME', scope='conv%d' % i, reuse=reuse)\n",
    "        net = max_pool(net, [2, 2], [2, 2], name='pool')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后我们把很多个不同的`vgg_block`堆叠在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(inputs, num_convs, out_depths, scope='vgg_stack', reuse=None):\n",
    "    \"\"\"构建vgg_stack.\n",
    "    \n",
    "    一个 vgg_stack 将若干个不同的`vgg_block`进行`stack`(堆叠)\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个block里卷积层的个数, 列表. 如`[1, 2, 3]`\n",
    "        out_depths: 每一个block的卷积核个数, 列表, 如`[64, 128, 256]`\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i, (n, d) in enumerate(zip(num_convs, out_depths)):\n",
    "            net = vgg_block(net, n, d, scope='block%d' % i)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最后我们在通过几个全连接层将`vgg`搭建完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(inputs, num_convs, out_depths, num_outputs, scope='vgg', reuse=None):\n",
    "    \"\"\"构建vgg.\n",
    "    \n",
    "    一个 vgg 先经过`vgg_stack`后再连接两个全连接层.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个 vgg_block 的卷积层的个数\n",
    "        out_depths: 每一个 vgg_block 卷积核个数\n",
    "        num_outputs: 最后输出向量的维数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = vgg_stack(inputs, num_convs, out_depths)\n",
    "        with tf.variable_scope('classification'):\n",
    "            net = tf.reshape(net, (batch_size, -1))\n",
    "            net = fc(net, 100, scope='fc1')\n",
    "            net = fc(net, num_outputs, act=tf.identity, scope='classification')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = vgg(train_imgs, (1, 1, 2, 2, 2), (64, 128, 256, 512, 512), 10)\n",
    "# 复用上面的参数\n",
    "val_out = vgg(val_imgs, (1, 1, 2, 2, 2), (64, 128, 256, 512, 512), 10, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失计算`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义正确率计算`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建训练`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 20000步的训练后, `VGG`在训练集和测试集分别达到了`0.97`和`0.75`的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小练习\n",
    "尝试用`Keras`或者`tf-slim`来实现`vgg`网络并训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
